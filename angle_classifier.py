# -*- coding: utf-8 -*-
"""angle_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tbDSJxLUeDF3bEA8eXFplBXNwfMrVvJN
"""

from google.colab import drive
drive.mount('/content/gdrive')

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import datetime
import os
import copy
from tqdm import tqdm

# init params
input_size = 224
output_class = 4
batch_size = 8
num_epoch = 20

# check GPU availability
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(input_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(input_size),
        transforms.CenterCrop(input_size),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = '/content/gdrive/MyDrive/classify_angle'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,
                                             shuffle=True, num_workers=2)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model_ft = models.mobilenet_v2(pretrained=True)
n_input_features = model_ft.classifier[1].in_features
model_ft.classifier[1] = nn.Linear(n_input_features, len(class_names))
model_ft.to(device)

# optimizer
optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)
# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
# optimizer = optim.Adam(model_ft.parameters(), lr=3e-4)
# loss function
criterion = nn.CrossEntropyLoss()

def fit(model, data_loader, criterion, optimizer, lr_scheduler, num_epochs=10):
    result = {'Train acc': [], 
              'Val acc': [],
              'Train loss': [],
              'Val loss': []}
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    train_acc_history = []
    val_acc_history = []
    for epoch in range(num_epochs):
        print("{} epoch {}:".format(datetime.datetime.now(),epoch))
        for phase in ['train', 'val']:
            running_loss = 0.0
            running_correct = 0
            if phase == 'train':
                model.train()
            else:
                model.eval()

            for data, target in data_loader[phase]:
                inputs = data.to(device)
                target = target.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, target)
                _, preds = torch.max(outputs, 1)
                if phase == 'train':
                    loss.backward()
                    optimizer.step()
                running_loss += loss.item()*inputs.size(0)
                running_correct += (preds == target).sum().item()
            epoch_loss = running_loss/len(data_loader[phase].dataset)
            epoch_accuracy = 100. * running_correct/len(data_loader[phase].dataset)
            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_accuracy))   
            if phase == 'val' and epoch_accuracy > best_acc:
                best_acc = epoch_accuracy
                best_model_wts = copy.deepcopy(model.state_dict())
                torch.save(model.state_dict(), '/content/gdrive/MyDrive/model/angle_classifier.pt')
            if phase == 'train':
                lr_scheduler.step()
                result['Train acc'].append(epoch_accuracy)
                result['Train loss'].append(epoch_loss)
            if phase == 'val':
                result['Val acc'].append(epoch_accuracy)
                result['Val loss'].append(epoch_loss)

        print("=============================================")
    model.load_state_dict(best_model_wts)
    return model, result

model_ft, result = fit(model_ft, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=20)

class_names

